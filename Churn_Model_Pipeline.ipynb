{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5051b733",
   "metadata": {},
   "source": [
    "# Customer Churn Modeling Pipeline\n",
    "\n",
    "This notebook walks through EDA, preprocessing, model training, and evaluation for predicting customer churn using the provided Excel workbook. Copy this notebook to the same folder as `Customer_Churn_Data_Large (1).xlsx` and run cells sequentially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0343b380",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "sns.set(style='whitegrid')\n",
    "pd.set_option('display.max_columns', 200)\n",
    "\n",
    "# Update filename if needed\n",
    "file_path = \"Customer_Churn_Data_Large (1).xlsx\"\n",
    "xls = pd.ExcelFile(file_path)\n",
    "print(\"Sheets available:\", xls.sheet_names)\n",
    "\n",
    "# Load sheets\n",
    "demographics = pd.read_excel(xls, sheet_name='Customer_Demographics')\n",
    "transactions = pd.read_excel(xls, sheet_name='Transaction_History')\n",
    "service = pd.read_excel(xls, sheet_name='Customer_Service')\n",
    "online = pd.read_excel(xls, sheet_name='Online_Activity')\n",
    "churn = pd.read_excel(xls, sheet_name='Churn_Status')\n",
    "\n",
    "print('\\nDemographics shape:', demographics.shape)\n",
    "print('Transactions shape:', transactions.shape)\n",
    "print('Service shape:', service.shape)\n",
    "print('Online activity shape:', online.shape)\n",
    "print('Churn shape:', churn.shape)\n",
    "\n",
    "# Quick peek\n",
    "display(demographics.head())\n",
    "display(transactions.head())\n",
    "display(service.head())\n",
    "display(online.head())\n",
    "display(churn.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1799ffdb",
   "metadata": {},
   "source": [
    "## EDA: Data types, missing values and basic stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aeb9a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Data types and missing values\n",
    "def df_summary(df, name):\n",
    "    print(f\"--- {name} ---\")\n",
    "    print(df.dtypes)\n",
    "    print('\\nMissing values:')\n",
    "    print(df.isnull().sum())\n",
    "    print('\\nSummary statistics:')\n",
    "    display(df.describe(include='all').T)\n",
    "    print('\\n\\n')\n",
    "\n",
    "df_summary(demographics, 'Demographics')\n",
    "df_summary(transactions, 'Transactions')\n",
    "df_summary(service, 'Service')\n",
    "df_summary(online, 'Online Activity')\n",
    "df_summary(churn, 'Churn')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad60d21",
   "metadata": {},
   "source": [
    "## EDA Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0173a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example visualizations\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.histplot(demographics['Age'].dropna(), kde=True, bins=30)\n",
    "plt.title('Age distribution'); plt.show()\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.countplot(x='IncomeLevel', data=demographics, order=demographics['IncomeLevel'].value_counts().index)\n",
    "plt.title('Income Level distribution'); plt.show()\n",
    "\n",
    "# Merge a small set to visualize churn by income\n",
    "demo_churn = demographics.merge(churn, on='CustomerID', how='left')\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.barplot(x='IncomeLevel', y='Churn', data=demo_churn, estimator=np.mean)\n",
    "plt.ylabel('Churn Rate'); plt.title('Churn rate by Income Level'); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd2e9ed",
   "metadata": {},
   "source": [
    "## Preprocessing & Feature Engineering\n",
    "- Merge datasets on CustomerID\n",
    "- Create aggregated transaction features (sum, mean, freq)\n",
    "- Create service features (counts, last contact etc.)\n",
    "- Handle missing values, encode categoricals, scale numeric features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe17052",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Merge datasets\n",
    "# Aggregate transactions per customer (example features)\n",
    "tx_agg = transactions.groupby('CustomerID').agg({\n",
    "    'TransactionAmount': ['sum', 'mean', 'count', 'max'],\n",
    "    'TransactionDate': ['min', 'max']\n",
    "}).reset_index()\n",
    "tx_agg.columns = ['CustomerID','tx_sum','tx_mean','tx_count','tx_max','tx_first_date','tx_last_date']\n",
    "\n",
    "# Service aggregation\n",
    "svc_agg = service.groupby('CustomerID').agg({\n",
    "    'ServiceCalls': 'sum',\n",
    "    'Complaints': 'sum',\n",
    "    'SatisfactionScore': 'mean'\n",
    "}).reset_index().rename(columns={'ServiceCalls':'svc_calls','Complaints':'svc_complaints','SatisfactionScore':'svc_sat_mean'})\n",
    "\n",
    "# Online activity aggregation (if present)\n",
    "if 'CustomerID' in online.columns:\n",
    "    on_agg = online.groupby('CustomerID').agg({\n",
    "        'PageViews': 'sum',\n",
    "        'Logins': 'sum',\n",
    "        'LastActiveDate': 'max'\n",
    "    }).reset_index().rename(columns={'PageViews':'on_pageviews','Logins':'on_logins','LastActiveDate':'on_last_active'})\n",
    "else:\n",
    "    on_agg = pd.DataFrame(columns=['CustomerID'])\n",
    "\n",
    "# Merge all\n",
    "df = demographics.merge(tx_agg, on='CustomerID', how='left')                 .merge(svc_agg, on='CustomerID', how='left')                 .merge(on_agg, on='CustomerID', how='left')                 .merge(churn, on='CustomerID', how='left')\n",
    "\n",
    "print('Merged dataset shape:', df.shape)\n",
    "display(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c9fea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Handle missing values: simple strategy with justification in markdown\n",
    "# Numeric: fillna with 0 or median; Categorical: fill with 'Unknown'\n",
    "num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "cat_cols = df.select_dtypes(include=['object','category']).columns.tolist()\n",
    "print('Numeric cols sample:', num_cols[:10])\n",
    "print('Categorical cols sample:', cat_cols[:10])\n",
    "\n",
    "# Fill numeric NaNs with 0 for monetary/usage aggregates, median for others\n",
    "for c in ['tx_sum','tx_mean','tx_count','tx_max','svc_calls','svc_complaints','svc_sat_mean','on_pageviews','on_logins']:\n",
    "    if c in df.columns:\n",
    "        df[c] = df[c].fillna(0)\n",
    "\n",
    "for c in cat_cols:\n",
    "    df[c] = df[c].fillna('Unknown')\n",
    "\n",
    "# Create recency and tenure features if date columns exist\n",
    "from datetime import datetime\n",
    "today = pd.to_datetime('2024-09-30')  # example reference date\n",
    "if 'tx_last_date' in df.columns and not df['tx_last_date'].isnull().all():\n",
    "    df['tx_last_date'] = pd.to_datetime(df['tx_last_date'])\n",
    "    df['tx_recency_days'] = (today - df['tx_last_date']).dt.days\n",
    "else:\n",
    "    df['tx_recency_days'] = -1\n",
    "\n",
    "if 'tx_first_date' in df.columns and not df['tx_first_date'].isnull().all():\n",
    "    df['tx_first_date'] = pd.to_datetime(df['tx_first_date'])\n",
    "    df['tx_tenure_days'] = (df['tx_last_date'] - df['tx_first_date']).dt.days.fillna(0)\n",
    "else:\n",
    "    df['tx_tenure_days'] = 0\n",
    "\n",
    "# Example ratio feature\n",
    "df['tx_amount_per_tx'] = df.apply(lambda row: row['tx_sum']/row['tx_count'] if row['tx_count']>0 else 0, axis=1)\n",
    "\n",
    "# Ensure target column name 'Churn' exists and is numeric 0/1\n",
    "df['Churn'] = df['Churn'].fillna(0).astype(int)\n",
    "\n",
    "display(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab8a356",
   "metadata": {},
   "source": [
    "### Encoding categorical variables and scaling numeric features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2872b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Encoding\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "# Select features for modeling\n",
    "features = df.drop(columns=['CustomerID','tx_first_date','tx_last_date'])\n",
    "\n",
    "# Separate target\n",
    "y = features['Churn']\n",
    "X = features.drop(columns=['Churn'])\n",
    "\n",
    "# Identify numeric and categorical\n",
    "numeric_feats = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "categorical_feats = X.select_dtypes(include=['object','category']).columns.tolist()\n",
    "\n",
    "print('Numeric features count:', len(numeric_feats))\n",
    "print('Categorical features count:', len(categorical_feats))\n",
    "\n",
    "# Simple pipeline using pandas\n",
    "X_encoded = pd.get_dummies(X, columns=categorical_feats, drop_first=True)\n",
    "\n",
    "# Scale numeric columns\n",
    "scaler = StandardScaler()\n",
    "X_encoded[numeric_feats] = scaler.fit_transform(X_encoded[numeric_feats])\n",
    "\n",
    "print('Final feature matrix shape:', X_encoded.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb7b71a",
   "metadata": {},
   "source": [
    "## Model training: Random Forest with cross-validation and GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a727b0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score, classification_report, confusion_matrix\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_encoded, y, test_size=0.2, random_state=42, stratify=y)\n",
    "print('Train/test sizes:', X_train.shape, X_test.shape)\n",
    "\n",
    "# Baseline Random Forest\n",
    "rf = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "baseline_scores = cross_val_score(rf, X_train, y_train, cv=cv, scoring='roc_auc')\n",
    "print('Baseline CV ROC-AUC:', baseline_scores.mean())\n",
    "\n",
    "# Hyperparameter search (small grid for speed)\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_leaf': [1, 3, 5]\n",
    "}\n",
    "grid = GridSearchCV(rf, param_grid, cv=cv, scoring='roc_auc', n_jobs=-1)\n",
    "grid.fit(X_train, y_train)\n",
    "print('Best params:', grid.best_params_)\n",
    "print('Best CV ROC-AUC:', grid.best_score_)\n",
    "\n",
    "best_rf = grid.best_estimator_\n",
    "# Evaluate on test\n",
    "y_pred = best_rf.predict(X_test)\n",
    "y_proba = best_rf.predict_proba(X_test)[:,1]\n",
    "print('Test ROC-AUC:', roc_auc_score(y_test, y_proba))\n",
    "print('\\nClassification report:\\n', classification_report(y_test, y_pred))\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print('\\nConfusion Matrix:\\n', cm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae66e9e6",
   "metadata": {},
   "source": [
    "## Feature importance & insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe26bd8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "importances = pd.Series(best_rf.feature_importances_, index=X_encoded.columns).sort_values(ascending=False)\n",
    "display(importances.head(30))\n",
    "\n",
    "# Plot top 15\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.barplot(x=importances.head(15), y=importances.head(15).index)\n",
    "plt.title('Top 15 Feature Importances'); plt.tight_layout(); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378fa4f5",
   "metadata": {},
   "source": [
    "## Recommendations & Next steps\n",
    "- Use model predictions to target high-risk customers with retention offers.\n",
    "- Monitor feature drift and retrain periodically.\n",
    "- Consider advanced models (XGBoost, LightGBM) and imbalance techniques (SMOTE, class weights).\n",
    "- Deploy model as batch scoring for retention campaigns and integrate with CRM."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
